What is Celery ?

*Celery is a task queue for executing work outside a python web application HTTP request-response cycle.
*A task queue's input is a unit of work called a task. Dedicated worker processes constantly monitor task
queues for new work to perform.

Why to use celery ?
* Third party api calling
* For high cpu intensive tasks.
* Periodic/Scheduled tasks.
* For improving user experience.


Celery requires a message transport to communicate between celery worker and the application\

* Celery requires a "message transport" to send and receive messages
* Supported brokers - RabbitMQ, Redis, Amazon SQS
* Result stores - AMQP, Redis, Memcached, Django ORM, Elasticsearch, MongoDB, Amazon S3, FileSystem
* Serialization - (Serialization for structuring data)
    ~ pickle, json, yaml, msgpack
    ~ zlib, bzip2 compression
    ~ Cryptographic message signing


* A celery system can consist of multiple workers and brokers,
giving way to high availability and horizontal scaling.



Worker
------
When you start a Celery worker on the command line via celery -- app=..., you just start a supervisor process.
The Celery worker itself does not process any tasks. It spawns child process (or threads) and deals with all the book keeping stuff.
The child processes (or threads) execute the actual tasks.The child processes (or threads) are also known as the execution pool.

The size of the execution pool determines the number of tasks your Celery worker can process concurrently at one single point of time.
    - But That totally depends on how many CPU cores your system has


Celery default use multiprocess concept by starting child process.

When Celery starts multiple child processes that's called as Pool of child processes of Pool.

No. of Child processes Celery Spawns or Create child processes  by seeing No. of CPU cores your system has



Worker -> Pool -> Concurrency
-----------------------------
When you start a celery worker, you specify the pool, concurrency, autoscale etc. in the command.

pool - decides who will actually perform the task - thread,child process,worker itself or else.

concurrency - concurrency will decide the size of pool.

autoscale - to dynamically resize the pool based on load. The autoscaler adds more pool processes when there is work to do,
and starts removing processes when the workload is low.

Command = $ celery -A <project> celery worker --pool=prefork --concurrency=5 --autoscale=10,3 - info


Instead of creating separate child processes we can create multiple threads

Should I use the pool of threads or Should I use the pool of child processes ?
    - That totally depends on the type of tasks you are performing.
    - If it's a CPU bound task then you will allocate task to the child process.
    - If most of the task are I/O based then recommend to use thread pool.

| prefork = Means we are using multiprocessing, or we are using child processes.
| Child processes basically means worker processes means we are creating multiple workers
| When we hit the command it crates one worker and this worker internally create multiple child processes
| Concurrency - When you have decided to use multiprocessing pool or worker processes pool, Now you will have to mention how large the pool should be
               How much child processes you want that your worker should create.
               - By default it has set to the no. of cores your system has but you can increase that or decrease that.
               - By increasing that there is no point of use because it only allocates the no of workers is equal to the cpu cores.

| autoscale - 10,3 means max 10 and min 3 . Autoscale can create more child processes up to 10. But only if we have 10 cpu cores

Celery supports following execution pool implementation
-------------------------------------------------------
*prefork (multiprocessing) [default]
*solo
*threads (multithreading)
*eventlet
*gevent

When you normally start the celery worker using-
$ celery -A <project>.celery worker -l info

By default, pool-> prefork and concurrency -> no. of cores


Prefork
--------
* Based on Python's multiprocessing package.
* Allows your celery worker to side-step Python's Global interpreter lock.
  and fully leverage multiple processors on a given machine.
* Use the prefork pool if your tasks are cpu bound.
* The number of available cores limits the number of concurrent processes.
* It only makes sense to run as many CPU tasks in parallel as there are CPUs available.
* That's why celery defaults to the number of CPUs available on the machine
  if the -concurrency argument is not set

Command -> $ celery -A <project>.celery worker -l info

Solo
----
* Neither threaded nor process based
* Not even a pool as it is always solo
* Contradicts the principle that the worker itself does not process any tasks
* The solo pool runs inside the worker process
* Runs inline which means there is no bookkeeping overhead
* This make the solo worker fast. But it also blocks the worker while it execute tasks.
* But it also blocks the worker while executes the tasks.
* In this concurrency doesn't make any sense.

Command -> celery -A <project>.celery worker --pool=solo -l info

Eventlet and gevent
-------------------
* Ex - To execute thousands of HTTP GET requests to fetch data from external REST APIs.
* The bottleneck is waiting for an input/Output operation to finish not CPU.
* Two thread-based execution pools; eventlet and gevent.
* To be precise, both eventlet and gevent use greenlets and not threads.
* There are implementation differences between the eventlet and gevent packages.

Command -
pip install gevent/eventlet
$ celery -A <project>.celery --pool=[gevent/eventlet] worker -l info

Difference between greenlets and threads.
-----------------------------------------
* Python's threading library makes use of the system's native OS to schedule
  threads. This general-purpose schedular is not always very efficient.
* It makes use of Python's global interpreter lock to make sure shared data
  structures are accessed by only one thread at a time to avoid race conditions.
* CPython interpreter , GIL, OS
* Greenlets emulate multithreaded environments without relying on any native operating system capabilities.
* Greenlets are managed application space and not in kernel space.
* In greenlets, no schedular pre-emptively switching between your threads at any given moment.
* Instead, Your greenlets voluntarily or explicitly give up control to one another at specified points in your code.
* Thus more scalable and efficient.
* Less RAM required


Threads
-------
* Multithreading concept
* Uses threading module of python
* Not much official support

Command -> celery -A <project>.celery worker --pool=threads -l info



Number of processes (multiprocessing/perfork pool)
-------------------------------------------------
More pool processes are usually better, but there's a cut-off point where adding more
pool processes affects performance in negetive ways. There's even some evidence to support that
having multiple worker instances running, May perform better than having a single worker.
For example 3 workers with 10 pool processes each. You need to experiment
to find the numbers that works best for you, as this varies based on application, work load,
task run times and factors.

Why multiple workers ?
----------------------
Running two celery workers on the same machine, each worker servicing a different queue
(in this case the queue names are "default" and "important")